import torch
import copy
from torch import nn
from torch.nn import functional as F
from torch import Tensor
from torch.nn.modules.linear import Linear
if float(torch.__version__.split('.')[0]) == 0 or (float(torch.__version__.split('.')[0]) == 1 and float(torch.__version__.split('.')[1])) < 9:
    from torch.nn.modules.linear import _LinearWithBias
else:
    from torch.nn.modules.linear import NonDynamicallyQuantizableLinear as _LinearWithBias
from torch.nn.modules.dropout import Dropout
import cv2 as cv
from torch.nn.init import constant_, xavier_normal_
from torch.nn.parameter import Parameter
from typing import  Optional, Tuple
from torch.nn import grad  # noqa: F401
from torch.nn.init import xavier_uniform_
from torch.nn.modules.normalization import LayerNorm
import os
from scipy.io import loadmat,savemat
import numpy as np
from torch.nn.modules.container import ModuleList
from sklearn.decomposition import PCA

# 改写MultiheadAttention，去掉对key的线性变换
class NewMultiheadAttention(nn.Module):
    __constants__ = ['batch_first']
    bias_k: Optional[torch.Tensor]
    bias_v: Optional[torch.Tensor]

    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,
                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(NewMultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.batch_first = batch_first
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"

        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
            self.register_parameter('in_proj_weight', None)
        else:
            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
            self.register_parameter('q_proj_weight', None)
            self.register_parameter('k_proj_weight', None)
            self.register_parameter('v_proj_weight', None)

        if bias:
            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))
        else:
            self.register_parameter('in_proj_bias', None)
        self.out_proj = _LinearWithBias(embed_dim, embed_dim)

        if add_bias_kv:
            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
        else:
            self.bias_k = self.bias_v = None

        self.add_zero_attn = add_zero_attn

        self._reset_parameters()

    def _reset_parameters(self):
        if self._qkv_same_embed_dim:
            xavier_uniform_(self.in_proj_weight)
        else:
            xavier_uniform_(self.q_proj_weight)
            xavier_uniform_(self.k_proj_weight)
            xavier_uniform_(self.v_proj_weight)

        if self.in_proj_bias is not None:
            constant_(self.in_proj_bias, 0.)
            constant_(self.out_proj.bias, 0.)
        if self.bias_k is not None:
            xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            xavier_normal_(self.bias_v)

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if '_qkv_same_embed_dim' not in state:
            state['_qkv_same_embed_dim'] = True

        super(NewMultiheadAttention, self).__setstate__(state)

    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,
                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:

        if self.batch_first:
            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]
        # k_proj_weight=self.k_proj_weight 改为 k_proj_weight=None
        if not self._qkv_same_embed_dim:
            attn_output, attn_output_weights = F.multi_head_attention_forward(
                query, key, value, self.embed_dim, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                self.bias_k, self.bias_v, self.add_zero_attn,
                self.dropout, self.out_proj.weight, self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask, need_weights=need_weights,
                attn_mask=attn_mask, use_separate_proj_weight=True,
                q_proj_weight=self.q_proj_weight, k_proj_weight=None,
                v_proj_weight=self.v_proj_weight)
        else:
            attn_output, attn_output_weights = F.multi_head_attention_forward(
                query, key, value, self.embed_dim, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                self.bias_k, self.bias_v, self.add_zero_attn,
                self.dropout, self.out_proj.weight, self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask, need_weights=need_weights,
                attn_mask=attn_mask)
        if self.batch_first:
            return attn_output.transpose(1, 0), attn_output_weights
        else:
            return attn_output, attn_output_weights


# 定3D_Gabor方法,目前是2d
def get_3d_gabor_filter(f, theta, phi, ratio, x, y, d):
    sigma = ratio / f

    u = f * np.sin(theta) * np.cos(phi)
    v = f * np.sin(theta) * np.sin(phi)
    w = f * np.cos(theta)

    [X, Y, Z] = np.meshgrid(np.arange(-x // 2 + 1, x // 2 + 1),
                            np.arange(-y // 2 + 1, y // 2 + 1),
                            np.arange(-d // 2 + 1, d // 2 + 1))

    prefix = 1 / (((2 * np.pi) ** 1.5) * sigma ** 3)
    gaussian = prefix * np.exp(-(X ** 2 + Y ** 2 + Z ** 2) / (2 * sigma ** 2))
    cosine = np.cos(2 * np.pi * (u * X + v * Y + w * Z))

    g_real = gaussian * cosine
    g_real = np.swapaxes(np.swapaxes(g_real, 0, 2), 1, 2)

    return g_real


def get_3d_gabor_filter_bank(nScale=1, M=13, x=3, y=3, d=13):
    f = 1 / (2 * nScale)

    theta = np.array([0, 1 / 4 * np.pi, 1 / 2 * np.pi, 3 / 4 * np.pi])
    phi = np.array([0, 1 / 4 * np.pi, 1 / 2 * np.pi, 3 / 4 * np.pi])

    g_filter_bank = np.zeros([M, d, x, y])
    counter = 0
    for i in range(4):
        for j in range(4):
            g_filter_bank[counter] = get_3d_gabor_filter(f=f, theta=theta[i], phi=phi[j], ratio=1, x=x, y=y, d=d)
            counter += 1
            if i == 0:
                break

    g_filter_bank = torch.from_numpy(np.float32(g_filter_bank))

    return g_filter_bank


def Gabor_3D(x:Tensor):
    # 生成gabor核 torch.Size([13, 1, 16, 5, 5])
    g = get_3d_gabor_filter_bank(nScale=1, M=13, x=5, y=5, d=10)
    g = g.unsqueeze(1)
    x = (Tensor.float(x)).unsqueeze(0).unsqueeze(0)
    x = nn.functional.conv3d(input=x, weight=g, padding=[0, 2, 2])
    x = x.squeeze(0).squeeze(1)
    return x


def Gabor_2D(x:Tensor):
    x = x.numpy()
    b = x.shape[0]
    for index_i in range(b):
        # paojie
        # retval = cv.getGaborKernel(ksize, sigma, theta, lambd, gamma[, psi[, ktype]])
        # Ksize 是一个元组
        retval = cv.getGaborKernel(ksize=(111, 111), sigma=10, theta=b, lambd=10, gamma=1.2)
        image1 = x[:, :, index_i]
        # dst   =   cv.filter2D(src, ddepth, kernel[, dst[, anchor[, delta[, borderType]]]])
        result = cv.filter2D(image1, -1, retval)
        x[:, :, index_i] = result
    x = torch.from_numpy(x)
    return x


def _get_activation_fn(activation):
    if activation == "relu":
        return F.relu
    elif activation == "gelu":
        return F.gelu

    raise RuntimeError("activation should be relu/gelu, not {}".format(activation))


def _get_clones(module, N):
    return ModuleList([copy.deepcopy(module) for i in range(N)])


# 定义nn.TransformerEncoderLayer子类对nn.TransformerEncoderLayer进行改写
class Transformer3D_GaborEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation="relu"):
        super(Transformer3D_GaborEncoderLayer, self).__init__()
        self.self_attn = NewMultiheadAttention(d_model, nhead, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = Linear(d_model, dim_feedforward)
        self.dropout = Dropout(dropout)
        self.linear2 = Linear(dim_feedforward, d_model)

        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.dropout1 = Dropout(dropout)
        self.dropout2 = Dropout(dropout)

        self.activation = _get_activation_fn(activation)

    def __setstate__(self, state):
        if 'activation' not in state:
            state['activation'] = F.relu
        super(Transformer3D_GaborEncoderLayer, self).__setstate__(state)

    def forward(self, x: Tensor, x_gabor: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
        r"""Pass the input through the encoder layer.

        Args:
            src: the sequence to the encoder layer (required).
            src_mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        src2 = self.self_attn(x, x_gabor, x, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = x + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src


class Transformer3D_GaborEncoder(nn.Module):
    __constants__ = ['norm']

    def __init__(self, encoder_layer, num_layers, norm=None):
        super(Transformer3D_GaborEncoder, self).__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(self, x: Tensor, x_gabor: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
        r"""Pass the input through the encoder layers in turn.

        Args:
            src: the sequence to the encoder (required).
            mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        output1 = x
        output2 = x_gabor
        for mod in self.layers:
            output1 = mod(output1, output2, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
        if self.norm is not None:
            output1 = self.norm(output1)

        return output1


class SpectralBranch(nn.Module):
    def __init__(self, c_in, embed_in, max_length, n_layer, skip, reduce=0.5):
        super().__init__()
        self.transform = nn.Linear(c_in, embed_in)
        self.transformband = nn.Linear(9*max_length, max_length)
        self.transform1 = nn.Linear(c_in, embed_in)
        self.pool77to55 = nn.AdaptiveAvgPool2d(output_size=(5, 5))
        self.pool55to33 = nn.AdaptiveAvgPool2d(output_size=(3, 3))

        self.pool1414to77 = nn.AdaptiveAvgPool2d(output_size=(7, 7))
        self.pool77to55_2 = nn.AdaptiveAvgPool2d(output_size=(5, 5))
        self.pool2828to1414 = nn.AdaptiveAvgPool2d(output_size=(14, 14))
        self.pool1414to77_3 = nn.AdaptiveAvgPool2d(output_size=(7, 7))

        self.pool33to77 = nn.AdaptiveAvgPool2d(output_size=(7, 7))
        self.pool55to77 = nn.AdaptiveAvgPool2d(output_size=(7, 7))
        self.pool1414to77_2 = nn.AdaptiveAvgPool2d(output_size=(7, 7))
        self.pool2828to77 = nn.AdaptiveAvgPool2d(output_size=(7, 7))
        self.feature = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(embed_in, 1, dim_feedforward=int(embed_in * reduce)), num_layers=n_layer
        )
        self.feature3D_Gabor = Transformer3D_GaborEncoder(
            Transformer3D_GaborEncoderLayer(embed_in, 1, dim_feedforward=int(embed_in * reduce)), num_layers=n_layer
        )

        self.cls_token = nn.Parameter(torch.FloatTensor(1, embed_in))
        self.pos_embed = nn.Embedding(max_length + 1, embed_in)
        self._reset_parameters()

        if skip:
            self.skip = True

    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.cls_token)
        nn.init.kaiming_uniform_(self.cls_token)

    def forward(self, x, x_gabor, x1, x2):
        batch = x.shape[0]
        x55_1 = self.pool77to55(x)
        x33_1 = self.pool55to33(x55_1)

        x77_1 = self.pool1414to77(x1)
        x55_2 = self.pool77to55_2(x77_1)

        x1414_1 = self.pool2828to1414(x2)
        x77_2 = self.pool1414to77_3(x1414_1)

        x33 = x33_1
        x55 = torch.cat([x55_1, x55_2], 1)
        x77 = torch.cat([x, x77_1, x77_2], 1)
        x1414 = torch.cat([x1, x1414_1], 1)
        x2828 = x2

        x77_3 = self.pool33to77(x33)
        x77_4 = self.pool55to77(x55)
        x77_5 = self.pool1414to77_2(x1414)
        x77_6 = self.pool2828to77(x2828)

        x77 = torch.cat([x77, x77_3, x77_4,x77_5,x77_6], 1)
        x = x77.reshape((batch, 7 ** 2, -1))
        x = self.transformband(x)
        x = x.permute(2, 0, 1)
        h = self.transform(x)
        cls_token = self.cls_token.expand(batch, -1).reshape((1, batch, -1))
        h = torch.cat([h, cls_token], dim=0)

        g = self.transform1(x_gabor)
        cls_token = self.cls_token.expand(batch, -1).reshape((1, batch, -1))
        g = torch.cat([g, cls_token], dim=0)
        position = torch.arange(h.shape[0]).repeat(batch).reshape((batch, -1)).to(h.device)
        h = h + self.pos_embed(position).permute(1, 0, 2)

        if hasattr(self, 'skip'):
            h = self.feature(h)
            h = h + self.feature3D_Gabor(h, g)
        else:
            h = self.feature(h)
            h = self.feature3D_Gabor(h, g)
        # h = self.feature(h)
        return h


class SpectralNet(nn.Module):
    def __init__(self, c_in, embed_in, nc, max_length, n_layer, skip=False, reduce=0.5):
        super().__init__()
        self.spectralBranch = SpectralBranch(c_in, embed_in, max_length, n_layer, skip, reduce)
        self.classifier = nn.Linear(embed_in, nc)

    def forward(self, x, x_gabor, x1, x2):
        h = self.spectralBranch(x, x_gabor, x1, x2)
        logits = self.classifier(h[0, :])
        return logits.squeeze(-1)


def entropy(hsi): #
    '''
    计算各波段的信息熵
    传入参数:hsi高光谱数据
    返回参数：ent各波段信息熵结果
    '''

    # 获取数据维度信息
    M, N, b = hsi.shape[0:3]
    # 定义ent变量存储信息熵结果
    ent = np.zeros(b)
    # 逐一波段求信息熵
    for i in range(b):
        # 获取当前波段数据
        band = hsi[:, :, i]
        band = band.reshape(N*M).tolist()
        # 定义频数、频率对应变量
        countDict = dict()
        proportitionDict = dict()
        # 利用集合去重
        band_set = set()
        for l in band:
            # print(l)
            band_set.add(l)
        # 计算信息熵
        for k in band_set:
            # 对集合每个非重复元素求得其在该波段的频数、频率
            countDict[k] = band.count(k)
            proportitionDict[k] = band.count(k)/len(band)
            # 根据信息熵公式求信息熵
            logp = np.log2(proportitionDict[k])
            ent[i] -= proportitionDict[k] * logp
    # 输出结果，并返回
    print(ent)
    print(len(ent))
    return ent


def selectBand(hsi, ent, threshold):
    '''
    波段选择：根据设置阈值，选择信息熵大于等于阈值的波段
    传入参数：hsi 高光谱数据,
            ent 各波段信息熵,
            threshold：设定阈值
    返回参数：selectband 选取的波段
            selecthsi 波段选择后的数据
    '''
    M, N, b = hsi.shape[0:3]
    selectband = list(range(b))
    for i in range(len(ent)):
        if ent[i] < threshold:
            selectband.remove(i)

    selecthsi = hsi[:, :, selectband]
    print('选取的波段为：', selectband)
    return selectband, selecthsi


def selectBandByVar(hsi, var, select_num):
    '''
    波段选择：根据设置阈值，选择信息熵大于等于阈值的波段
    传入参数：hsi 高光谱数据,
            var 各波段方差,
            select_num：设定选择波段数
    返回参数：selectband 选取的波段
            selecthsi 波段选择后的数据
    '''
    M, N, b = hsi.shape[0:3]
    selectband = []
    var = var.tolist()
    print('var:', var)
    sorted_var = sorted(var, reverse = True)
    print('sorted_var:', sorted_var)
    for i in range(select_num):
        selectband.append(var.index(sorted_var[i]))

    selecthsi = hsi[:, :, selectband]
    print('选取的波段为：', selectband)
    print('选取的波段方差为：', sorted_var)
    return selectband, selecthsi


def pcadata(x, band_num):
    h,w = x.shape[0:2]
    # PCA 降维
    pca = PCA(60, whiten=True)
    x_pca = pca.fit_transform(x.reshape(h * w, -1))
    x_pca = x_pca.reshape(h, w, -1)
    return x_pca

# if __name__ == '__main__':
#     x = torch.rand(64,112,7,7)
#     x1 = torch.rand(64, 112, 14, 14)
#     x2 = torch.rand(64, 112, 28, 28)
#     x_gabor = torch.rand(60, 64, 49)
#     net = SpectralBranch(49, 64, 112, 3)
#     output = net(x, x_gabor, x1, x2)


